# -*- coding: utf-8 -*-
"""BTP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hBb-QbNYXVCguZsWJWtkVumfrZ2mrsq5
"""

!pip install torch torchvision torchaudio
!pip install ultralytics

import time

while True:
    time.sleep(60)
    print("Keeping the runtime alive...")

from google.colab import drive
drive.mount('/content/drive')

import os
from glob import glob

# Replace with the actual path to your dataset in Google Drive
dataset_path_inside = '/content/drive/MyDrive/inside/images*/*'
dataset_path_outside = '/content/drive/MyDrive/outside/images*/*'


image_paths_inside = glob(dataset_path_inside)
image_paths_outside = glob(dataset_path_outside)

print(f"Number of images inside the folder: {len(image_paths_inside)}")
print(f"Number of images outside the folder: {len(image_paths_outside)}")

labels_path_inside = '/content/drive/MyDrive/inside/Annotations_XML/*.xml'
labels_path_outside = '/content/drive/MyDrive/outside/Annotations_XML/*.xml'

label_paths_inside = glob(labels_path_inside)
label_paths_outside = glob(labels_path_outside)

print(f"Number of label files inside the folder: {len(label_paths_inside)}")
print(f"Number of label files outside the folder: {len(label_paths_outside)}")

!pip install Pillow

from PIL import Image

# Function to check the size of an image
def check_image_size(image_paths):
    for img_path in image_paths:
        with Image.open(img_path) as img:
            # Print the size of the image
            print(f"Image: {img_path}, Size: {img.size}")

# Check image sizes for inside and outside images
print("Inside Folder Image Sizes:")
check_image_size(image_paths_inside[:5])  # Check the first 5 images as a sample

print("\nOutside Folder Image Sizes:")
check_image_size(image_paths_outside[:5])  # Check the first 5 images as a sample

from PIL import Image
import matplotlib.pyplot as plt


def display_image(image_path):
    with Image.open(image_path) as img:

        plt.imshow(img)
        plt.axis('off')
        plt.show()


image_path = '/content/drive/MyDrive/inside/images/inside13.jpg'
display_image(image_path)

import multiprocessing

# Check the number of CPU cores
cpu_cores = multiprocessing.cpu_count()
print(f"Available CPU cores: {cpu_cores}")

from multiprocessing import Pool
from PIL import Image
import os


def resize_image(img_path_output):
    img_path, output_folder, size = img_path_output
    with Image.open(img_path) as img:

        img_resized = img.resize(size)
        img_name = os.path.basename(img_path)
        img_resized.save(os.path.join(output_folder, img_name))


def resize_images_multiprocessing(image_paths, output_folder, size=(320, 320), num_workers=2):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)


    img_args = [(img_path, output_folder, size) for img_path in image_paths]


    with Pool(num_workers) as p:
        p.map(resize_image, img_args)


resized_inside_folder = '/content/dataset/resized/inside'
resized_outside_folder = '/content/dataset/resized/outside'


resize_images_multiprocessing(image_paths_inside, resized_inside_folder, num_workers=4)
resize_images_multiprocessing(image_paths_outside, resized_outside_folder, num_workers=4)

print("Image resizing completed.")

from sklearn.model_selection import train_test_split
import os
import shutil

# List all image files in the resized inside and outside folders
image_files_inside = os.listdir(resized_inside_folder)
image_files_outside = os.listdir(resized_outside_folder)

# Label the image files to indicate their source (inside or outside)
image_files_inside = [(img, 'inside') for img in image_files_inside]
image_files_outside = [(img, 'outside') for img in image_files_outside]

# Combine the images from inside and outside
all_image_files = image_files_inside + image_files_outside

# Split into 70% train, 20% test, 10% validation
train_images, temp_images = train_test_split(all_image_files, test_size=0.3, random_state=42)
val_images, test_images = train_test_split(temp_images, test_size=0.67, random_state=42)  # 33% of 30% is 10%

# Create directories for train, test, and validation images and labels
os.makedirs('/content/dataset/images/train', exist_ok=True)
os.makedirs('/content/dataset/images/test', exist_ok=True)
os.makedirs('/content/dataset/images/val', exist_ok=True)
os.makedirs('/content/dataset/labels/train', exist_ok=True)
os.makedirs('/content/dataset/labels/test', exist_ok=True)
os.makedirs('/content/dataset/labels/val', exist_ok=True)

# Function to move images and corresponding labels
def move_files(image_list, inside_folder, outside_folder, target_image_folder, target_label_folder):
    for image, source_folder in image_list:
        if source_folder == 'inside':
            source_image_path = f'{inside_folder}/{image}'
        else:
            source_image_path = f'{outside_folder}/{image}'

        # Move the image
        shutil.move(source_image_path, target_image_folder)

        # Move the corresponding label
        label_file = image.replace('.jpg', '.txt')  # Assuming the image files are .jpg
        label_source_path = f'/content/dataset/labels/train/{label_file}'  # Adjust if needed

        if os.path.exists(label_source_path):
            shutil.move(label_source_path, target_label_folder)

# Move train images and labels
move_files(train_images, resized_inside_folder, resized_outside_folder, '/content/dataset/images/train', '/content/dataset/labels/train')

# Move test images and labels
move_files(test_images, resized_inside_folder, resized_outside_folder, '/content/dataset/images/test', '/content/dataset/labels/test')

# Move validation images and labels
move_files(val_images, resized_inside_folder, resized_outside_folder, '/content/dataset/images/val', '/content/dataset/labels/val')

print("Dataset successfully split into train, test, and validation sets.")

!pip install ultralytics
!pip install xmltodict

import xmltodict
import os
class_names = ['fire', 'smoke']

# Function to convert XML annotations to YOLO format
def convert_xml_to_yolo(xml_file, output_dir):
    with open(xml_file) as f:
        data = xmltodict.parse(f.read())

    image_width = int(data['annotation']['size']['width'])
    image_height = int(data['annotation']['size']['height'])

    objects = data['annotation'].get('object', [])
    if isinstance(objects, dict):
        objects = [objects]  # Handle the case where there's only one object in the XML

    yolo_lines = []
    for obj in objects:
        class_name = obj['name']
        if class_name not in class_names:
            continue  # Ignore objects not in our class list

        class_id = class_names.index(class_name)

        xmin = int(obj['bndbox']['xmin'])
        ymin = int(obj['bndbox']['ymin'])
        xmax = int(obj['bndbox']['xmax'])
        ymax = int(obj['bndbox']['ymax'])

        # Convert to YOLO format (normalized x_center, y_center, width, height)
        x_center = ((xmin + xmax) / 2) / image_width
        y_center = ((ymin + ymax) / 2) / image_height
        width = (xmax - xmin) / image_width
        height = (ymax - ymin) / image_height

        yolo_lines.append(f"{class_id} {x_center} {y_center} {width} {height}")

    # Write YOLO formatted labels to a .txt file
    output_file = os.path.join(output_dir, os.path.basename(xml_file).replace('.xml', '.txt'))
    with open(output_file, 'w') as out_f:
        out_f.write("\n".join(yolo_lines))

# Create directories to store the converted labels
os.makedirs('/content/dataset/labels/train', exist_ok=True)

# Convert all XML files inside the inside and outside folders
for xml_file in label_paths_inside:
    convert_xml_to_yolo(xml_file, '/content/dataset/labels/train')

for xml_file in label_paths_outside:
    convert_xml_to_yolo(xml_file, '/content/dataset/labels/train')

print("Conversion complete!")



# Define YAML configuration for YOLOv8
yaml_content = """
path: /content/dataset  # Dataset root directory
train: images/train  # Train images (relative to 'path')
val: images/val  # Validation images (relative to 'path')
test: images/test  # Test images (optional)

nc: 2  # Number of classes (fire and smoke)
names: ['fire', 'smoke']  # Class names
"""

# Write the content to a YAML file
with open('/content/fire_smoke.yaml', 'w') as yaml_file:
    yaml_file.write(yaml_content)

print("YAML configuration file created!")

from ultralytics import YOLO

# Load the YOLOv8n model pretrained on COCO
model = YOLO('yolov8n.pt')

# Train the model using faster tuning settings
model.train(
    data='/content/fire_smoke.yaml',  # Your dataset YAML file
    epochs=10,                       # Lower epochs for faster training
    imgsz=320,                        # Smaller image size for speed
    batch=16,                         # Suitable batch size for your CPU
    cache=True,                       # Enable caching for faster dataset loading
    workers=2,                        # Reduce workers to prevent CPU overload
    optimizer='Adam',                 # Adam optimizer is faster for tuning
    freeze=10,                        # Freeze backbone layers for faster training
    verbose=False,                    # Disable verbose logging to speed up training
    patience=20,                      # Early stopping after 20 epochs of no improvement
    amp=True                          # Enable automatic mixed precision to reduce memory usage
)

# Commented out IPython magic to ensure Python compatibility.

# %load_ext tensorboard
# %tensorboard --logdir runs/detect

results = model.val()
print(results)  # This will print precision, recall, mAP metrics for the test set

!cp runs/detect/train/weights/best.pt /content/drive/MyDrive/best_yolov8n_model.pt

































